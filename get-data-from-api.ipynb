{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "1. COVID-19 World Survey Data API https://covidmap.umd.edu/api.html\n",
    "2. 공공데이터포털 끌어오기 https://greendreamtrre.tistory.com/268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "#!pip install lxml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import requests, bs4\n",
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime as dt \n",
    "from lxml import html\n",
    "import urllib \n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode, quote_plus, unquote\n",
    "import itertools \n",
    "#\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From json \n",
    "\n",
    "- 제이슨 파일에서 끌어오는 방식이다. \n",
    "- 각각 url별로 국가, 지역레벨 등등으로 나뉘어져 있다. \n",
    "- url에 옵션을 줘서 데이터를 끌어온다. 이것이 `get` 방식이다. \n",
    "- json을 pd.DataFrame으로 바꾸는 건 상대적으로 용이한 듯. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df(url_level, country=\"South%Korea\", region=\"all\", date=\"20200506\"):\n",
    "    # request data from api\n",
    "    url_api_0 = \"https://covidmap.umd.edu/api/country\" # get country-level \n",
    "    url_api_1 = \"https://covidmap.umd.edu/api/region\" # get region-level \n",
    "    url_api_2 = f\"https://covidmap.umd.edu/api/datesavail?country={country}\"\n",
    "    url_api_3 = f\"https://covidmap.umd.edu/api/resources?indicator=covid&type=daily&country={country}&region={region}&date={date}\"\n",
    "    this_api = eval(f'url_api_{url_level}')\n",
    "    response = requests.get(this_api, verify=False).text\n",
    "    jsonData = json.loads(response)\n",
    "    return jsonData['data']\n",
    "    #return pd.DataFrame.from_dict(jsonData['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(dirty_list): \n",
    "    tlist1 = list(itertools.chain(*dirty_list)) \n",
    "    return pd.DataFrame(tlist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fb_survey(dates): \n",
    "    \n",
    "    dates_len = len(dates)\n",
    "    res = [] \n",
    "    q = 0\n",
    "    \n",
    "    for date in dates:        \n",
    "        q += 1\n",
    "        list_appd = retrieve_df(url_level=3, date=date)         \n",
    "        res.append(list_appd)\n",
    "        print(f\"Write df for {date}, and {q} of {dates_len}\")\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    return clean_list(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write df for 20201013, and 173 of 173\n"
     ]
    }
   ],
   "source": [
    "dates =[k['survey_date'] for k in retrieve_df(2)] \n",
    "df_fb = gen_fb_survey(dates)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "df_fb_seoul = df_fb.query('`region` == \"Seoul\"')\n",
    "#df_fb_seoul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서울시 api에서 끌어오기 \n",
    "\n",
    "- 일단 친절하지 않다. \n",
    "- 방식은 페이지 넘버를 통해 데이터를 끌어온다. \n",
    "- 역시 제이슨이기 때문에 쉽게 바꿀 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datestring(input_str):\n",
    "    \n",
    "    res1 = input_str.rjust(6, '0')\n",
    "    return f\"2020.{res1}\".replace(\".\", \"\")\n",
    "\n",
    "def generate_covid_seoul(start_page):\n",
    "    # url 변수에 최종 완성본 url을 넣자\n",
    "    end_page = start_page + 999\n",
    "    url = f\"http://openapi.seoul.go.kr:8088/7067764353616e6137394f68524844/json/Corona19Status/{start_page}/{end_page}\"\n",
    "     # url을 불러오고 이것을 인코딩을 utf-8로 전환하여 결과를 받자.\n",
    "    response = urllib.request.urlopen(url) \n",
    "    json_str = response.read().decode(\"utf-8\")\n",
    "    # 받은 데이터가 문자열이라서 이를 json으로 변환한다.\n",
    "    json_object = json.loads(json_str)\n",
    "    json_to_dict = json_object['Corona19Status']['row']\n",
    "    tdf = pd.DataFrame.from_dict(json_to_dict)\n",
    "    tdf['date_reported'] = tdf['CORONA19_DATE'].apply(make_datestring)\n",
    "    \n",
    "    #return url \n",
    "    return tdf.groupby(['date_reported']).size().to_frame(name = 'count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_seoul = pd.DataFrame() \n",
    "\n",
    "for i in range(1, 6001, 1000): \n",
    "    tdfa = generate_covid_seoul(i)\n",
    "    df_covid_seoul = pd.concat([df_covid_seoul, tdfa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_fb_seoul, df_covid_seoul, how='left', left_on='survey_date', right_on='date_reported') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gc = df.query('`date_reported` == `date_reported`')\n",
    "#df_gc.columns\n",
    "#df_gc['cli_raw'] = df_gc['sample_size'] * df_gc['percent_cli_unw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xml에서 끌어오기 \n",
    "\n",
    "- 살짝 복잡하지만 그렇게 어렵지는 않다. \n",
    "- 역시 get방식이다. \n",
    "- query parameter를 정형화해두어서 보기에 편하다. \n",
    "- 나머지는 다소 기계적으로 수행이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. URL 파라미터 분리하기.\n",
    "# Service URL\n",
    "xmlUrl = 'http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19SidoInfStateJson'\n",
    "My_API_Key = unquote('M3jBsz%2FCY6CB9tMYmAf9SQ8AMW6AtjQDjshDbBlpsTadmBOfyBTVoYSqAvMVX5HH2GNgM%2FzOHv150PFIyQbkig%3D%3D')    # 아래 내가 받은 인증키가 안 되서 수업용 인증키 사용.\n",
    "\n",
    "def get_xml_from_api(url, from_date, to_date, api_key=My_API_Key):\n",
    "\n",
    "    queryParams = '?' + urlencode(    # get 방식으로 쿼리를 분리하기 위해 '?'를 넣은 것이다. 메타코드 아님.\n",
    "        {\n",
    "            quote_plus('ServiceKey') : My_API_Key,    # 필수 항목 1 : 서비스키 (본인의 서비스키)\n",
    "            quote_plus('startCreateDt') : '20200101',          # 필수 항목 2 : 지역코드 (법정코드목록조회에서 확인)\n",
    "            quote_plus('endCreateDt') :   '20201015'         # 픽수 항목 3 : 계약월\n",
    "         }\n",
    "    )\n",
    "    response = requests.get(xmlUrl + queryParams).text.encode('utf-8')\n",
    "    return bs4.BeautifulSoup(response, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xml_into_df(xmlobj):\n",
    "    rows = xmlobj.findAll('item')\n",
    "    rowList = []\n",
    "    nameList = []\n",
    "    columnList = []\n",
    "\n",
    "    rowsLen = len(rows)\n",
    "    for i in range(0, rowsLen):\n",
    "        columns = rows[i].find_all()\n",
    "    \n",
    "        columnsLen = len(columns)\n",
    "        for j in range(0, columnsLen):\n",
    "            # 첫 번째 행 데이터 값 수집 시에만 컬럼 값을 저장한다. (어차피 rows[0], rows[1], ... 모두 컬럼헤더는 동일한 값을 가지기 때문에 매번 반복할 필요가 없다.)\n",
    "            if i == 0:\n",
    "                nameList.append(columns[j].name)\n",
    "            # 컬럼값은 모든 행의 값을 저장해야한다.    \n",
    "            eachColumn = columns[j].text\n",
    "            columnList.append(eachColumn)\n",
    "        rowList.append(columnList)\n",
    "        columnList = []    # 다음 row의 값을 넣기 위해 비워준다. (매우 중요!!)\n",
    "    \n",
    "    return pd.DataFrame(rowList, columns=nameList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = get_xml_from_api(xmlUrl, \"20200101\", \"20201016\")\n",
    "tdf = convert_xml_into_df(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_seoul = tdf.query('gubun == \"서울\"')[['createdt', 'incdec']]\n",
    "df_covid_seoul['date'] = df_covid_seoul['createdt'].apply(lambda x: str(x).split(\" \")[0].replace(\"-\", \"\"))\n",
    "df = df_fb_seoul[['survey_date','percent_cli']].merge(df_covid_seoul[['date', 'incdec']], how='left', left_on=\"survey_date\", right_on='date')\n",
    "df['incdec'] = df['incdec'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey_date</th>\n",
       "      <th>percent_cli</th>\n",
       "      <th>date</th>\n",
       "      <th>incdec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200503</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>20200503</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200504</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>20200504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200505</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>20200505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200506</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>20200506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200507</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>20200507</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>20201007</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>20201007</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>20201009</td>\n",
       "      <td>0.014884</td>\n",
       "      <td>20201009</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>20201012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20201012</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>20201012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20201012</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20201013</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>20201013</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    survey_date  percent_cli      date  incdec\n",
       "0      20200503     0.004424  20200503       2\n",
       "1      20200504     0.003877  20200504       0\n",
       "2      20200505     0.002137  20200505       0\n",
       "3      20200506     0.010908  20200506       0\n",
       "4      20200507     0.008095  20200507       0\n",
       "..          ...          ...       ...     ...\n",
       "161    20201007     0.004660  20201007      33\n",
       "162    20201009     0.014884  20201009      22\n",
       "163    20201012     0.000000  20201012      31\n",
       "164    20201012     0.000000  20201012      31\n",
       "165    20201013     0.001849  20201013      20\n",
       "\n",
       "[166 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import statsmodels.api as sm\n",
    ">>> from statsmodels.tsa.stattools import grangercausalitytests\n",
    ">>> import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['incdec', 'percent_cli', 'date']]\n",
    "#data['d_percent_cli'] = df['percent_cli'].diff()\n",
    "#data['pct_incdec'] = df['incdec'].pct_change()\n",
    "#data['pct_incdec'] = data['pct_incdec'].apply(lambda x: np.nan_to_num(x))\n",
    "#data = data.applymap(lambda x: np.nan_to_num(x))\n",
    "#gdata = data.loc[1:,['pct_incdec','d_percent_cli']]\n",
    "gdata = data[['percent_cli', 'incdec']]\n",
    "gdata = data[['incdec','percent_cli']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.0007  , p=0.9791  , df_denom=162, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0007  , p=0.9789  , df=1\n",
      "likelihood ratio test: chi2=0.0007  , p=0.9789  , df=1\n",
      "parameter F test:         F=0.0007  , p=0.9791  , df_denom=162, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=0.0524  , p=0.9489  , df_denom=159, df_num=2\n",
      "ssr based chi2 test:   chi2=0.1082  , p=0.9474  , df=2\n",
      "likelihood ratio test: chi2=0.1081  , p=0.9474  , df=2\n",
      "parameter F test:         F=0.0524  , p=0.9489  , df_denom=159, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 3\n",
      "ssr based F test:         F=0.0458  , p=0.9869  , df_denom=156, df_num=3\n",
      "ssr based chi2 test:   chi2=0.1437  , p=0.9861  , df=3\n",
      "likelihood ratio test: chi2=0.1436  , p=0.9861  , df=3\n",
      "parameter F test:         F=0.0458  , p=0.9869  , df_denom=156, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 4\n",
      "ssr based F test:         F=0.9996  , p=0.4097  , df_denom=153, df_num=4\n",
      "ssr based chi2 test:   chi2=4.2335  , p=0.3753  , df=4\n",
      "likelihood ratio test: chi2=4.1791  , p=0.3823  , df=4\n",
      "parameter F test:         F=0.9996  , p=0.4097  , df_denom=153, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 5\n",
      "ssr based F test:         F=0.7479  , p=0.5888  , df_denom=150, df_num=5\n",
      "ssr based chi2 test:   chi2=4.0140  , p=0.5474  , df=5\n",
      "likelihood ratio test: chi2=3.9647  , p=0.5545  , df=5\n",
      "parameter F test:         F=0.7479  , p=0.5888  , df_denom=150, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 6\n",
      "ssr based F test:         F=0.7745  , p=0.5911  , df_denom=147, df_num=6\n",
      "ssr based chi2 test:   chi2=5.0583  , p=0.5364  , df=6\n",
      "likelihood ratio test: chi2=4.9800  , p=0.5464  , df=6\n",
      "parameter F test:         F=0.7745  , p=0.5911  , df_denom=147, df_num=6\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 7\n",
      "ssr based F test:         F=0.7244  , p=0.6515  , df_denom=144, df_num=7\n",
      "ssr based chi2 test:   chi2=5.5989  , p=0.5873  , df=7\n",
      "likelihood ratio test: chi2=5.5026  , p=0.5989  , df=7\n",
      "parameter F test:         F=0.7244  , p=0.6515  , df_denom=144, df_num=7\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 8\n",
      "ssr based F test:         F=0.7154  , p=0.6776  , df_denom=141, df_num=8\n",
      "ssr based chi2 test:   chi2=6.4130  , p=0.6011  , df=8\n",
      "likelihood ratio test: chi2=6.2863  , p=0.6152  , df=8\n",
      "parameter F test:         F=0.7154  , p=0.6776  , df_denom=141, df_num=8\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 9\n",
      "ssr based F test:         F=0.6287  , p=0.7710  , df_denom=138, df_num=9\n",
      "ssr based chi2 test:   chi2=6.4370  , p=0.6955  , df=9\n",
      "likelihood ratio test: chi2=6.3086  , p=0.7087  , df=9\n",
      "parameter F test:         F=0.6287  , p=0.7710  , df_denom=138, df_num=9\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 10\n",
      "ssr based F test:         F=0.7018  , p=0.7215  , df_denom=135, df_num=10\n",
      "ssr based chi2 test:   chi2=8.1095  , p=0.6181  , df=10\n",
      "likelihood ratio test: chi2=7.9058  , p=0.6380  , df=10\n",
      "parameter F test:         F=0.7018  , p=0.7215  , df_denom=135, df_num=10\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 11\n",
      "ssr based F test:         F=0.6930  , p=0.7434  , df_denom=132, df_num=11\n",
      "ssr based chi2 test:   chi2=8.9513  , p=0.6264  , df=11\n",
      "likelihood ratio test: chi2=8.7024  , p=0.6493  , df=11\n",
      "parameter F test:         F=0.6930  , p=0.7434  , df_denom=132, df_num=11\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 12\n",
      "ssr based F test:         F=0.6767  , p=0.7713  , df_denom=129, df_num=12\n",
      "ssr based chi2 test:   chi2=9.6937  , p=0.6428  , df=12\n",
      "likelihood ratio test: chi2=9.4008  , p=0.6684  , df=12\n",
      "parameter F test:         F=0.6767  , p=0.7713  , df_denom=129, df_num=12\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 13\n",
      "ssr based F test:         F=0.6207  , p=0.8332  , df_denom=126, df_num=13\n",
      "ssr based chi2 test:   chi2=9.7987  , p=0.7103  , df=13\n",
      "likelihood ratio test: chi2=9.4977  , p=0.7344  , df=13\n",
      "parameter F test:         F=0.6207  , p=0.8332  , df_denom=126, df_num=13\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 14\n",
      "ssr based F test:         F=0.6115  , p=0.8514  , df_denom=123, df_num=14\n",
      "ssr based chi2 test:   chi2=10.5793 , p=0.7187  , df=14\n",
      "likelihood ratio test: chi2=10.2274 , p=0.7454  , df=14\n",
      "parameter F test:         F=0.6115  , p=0.8514  , df_denom=123, df_num=14\n"
     ]
    }
   ],
   "source": [
    "res = grangercausalitytests(gdata, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
